{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1521f9e1",
   "metadata": {},
   "source": [
    "# Build model without dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9d02a6",
   "metadata": {},
   "source": [
    "## This model is paragraph_based sentenec classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6e998b",
   "metadata": {},
   "source": [
    "### Import libaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9dae318a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0379e4d2",
   "metadata": {},
   "source": [
    "### Create a dummy dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a707001c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame({\n",
    "    'text': [\n",
    "        \"This is not spam.\",\n",
    "        \"It's a legitimate message.\",\n",
    "        \"Check out this amazing offer! You won't believe it.\",\n",
    "        \"Hello, how are you? Hope you're doing well.\",\n",
    "        \"Win a free iPhone now! Claim your prize!\",\n",
    "        \"Congratulations, you've won $1,000,000! Click to claim your winnings.\",\n",
    "        \"This is not spam.\",\n",
    "        \"Check out this amazing offer!\",\n",
    "        \"Hello, how are you?\",\n",
    "        \"Win a free iPhone now!\",\n",
    "        \"Congratulations, you've won $1,000,000.\",\n",
    "        \"you've won 1,000,000rs.\",\n",
    "        \"Get a 50% discount on all products today!\",\n",
    "        \"Limited-time offer: Buy one, get one free!\",\n",
    "        \"Exclusive deal: Save $100 on your purchase.\",\n",
    "        \"Hello, it's me! Are you there?\",\n",
    "        \"Earn money from home with our easy work-from-home program!\",\n",
    "        \"You're the lucky winner of a luxury vacation!\",\n",
    "        \"Hurry! Limited stock available for our new product launch.\",\n",
    "        \"Greetings, hope you're having a great day!\",\n",
    "        \"Claim your prize now, don't miss out on this opportunity!\",\n",
    "        \"Congratulations, you've been selected for a special offer!\",\n",
    "        \"Guaranteed to lose weight fast with our new product!\",\n",
    "        \"Dear customer, your account needs attention.\",\n",
    "        \"Amazing deals on electronics! Shop now!\",\n",
    "        \"You're pre-approved for a credit card with a $10,000 limit!\",\n",
    "        \"Click here to unsubscribe from our mailing list.\",\n",
    "        \"Important: Your account security has been compromised.\",\n",
    "        \"Join our loyalty program and get exclusive discounts!\",\n",
    "    ],\n",
    "    'label': [1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1]  # 0 for not spam, 1 for spam\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e4db3b",
   "metadata": {},
   "source": [
    "### Preprocess the data with NLTK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebc0f13a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the data\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941b59a5",
   "metadata": {},
   "source": [
    "### Split the paragraph into sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59b86818",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph = \"My Grandfather smoked his whole life. I was about 10 years old when my mother said to him, 'If you ever want to see your grandchildren graduate, you have to stop immediately.'. Tears welled up in his eyes when he realized what exactly was at stake. He gave it up immediately. Three years later he died of lung cancer. It was really sad and destroyed me. My mother said to me- 'Don't ever smoke. Please don't put your family through what your Grandfather put us through. I agreed. At 28, I have never touched a cigarette. I must say, I feel a very slight sense of regret for never having done it, because your post gave me cancer anyway.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b3c83b",
   "metadata": {},
   "source": [
    "### Creat a sentence vectorizer object CountVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35a6ccb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = sent_tokenize(paragraph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dcc7c4d",
   "metadata": {},
   "source": [
    "### Initialize a CountVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2200acf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7aab669",
   "metadata": {},
   "source": [
    "### Fit and transform the data for training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ef1676a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_vectorized = vectorizer.fit_transform(data['text'])\n",
    "y_train = data['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15f632f",
   "metadata": {},
   "source": [
    "### Train a Naive Bayes classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d2aa4f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = MultinomialNB()\n",
    "clf.fit(X_train_vectorized, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f5f814",
   "metadata": {},
   "source": [
    "### Initialize variables to store information about each sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42856041",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_info = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4ce5d4",
   "metadata": {},
   "source": [
    "### Define spam words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ac4c848",
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_words = [\n",
    "    'free',\n",
    "    'win',\n",
    "    'prize',\n",
    "    'million',\n",
    "    'lottery',\n",
    "    'guaranteed',\n",
    "    'cash',\n",
    "    'money',\n",
    "    'credit',\n",
    "    'loan',\n",
    "    'viagra',\n",
    "    'pharmacy',\n",
    "    'mortgage',\n",
    "    'discount',\n",
    "    'offer',\n",
    "    'debt',\n",
    "    'investment',\n",
    "    'earn',\n",
    "    'income',\n",
    "    'online',\n",
    "    'limited time',\n",
    "    'risk-free',\n",
    "    'urgent',\n",
    "    'click below',\n",
    "    'buy direct',\n",
    "    'no catch',\n",
    "    'no cost',\n",
    "    'no fees',\n",
    "    'save big money',\n",
    "    'order now',\n",
    "    'apply now',\n",
    "    'get it now',\n",
    "    'call now',\n",
    "    'meet singles',\n",
    "    'meet hot singles',\n",
    "    'singles near you',\n",
    "    'meet sexy singles',\n",
    "    'get out of debt',\n",
    "    'insurance',\n",
    "    'warranty',\n",
    "    'obligation',\n",
    "    'reverses aging',\n",
    "    'hidden',\n",
    "    'prizes',\n",
    "    'promise',\n",
    "    '100% satisfied',\n",
    "    'money-back',\n",
    "    'stop',\n",
    "    'lose',\n",
    "    'miracle',\n",
    "    'mass email',\n",
    "    'full refund',\n",
    "    'no hidden',\n",
    "    'investment',\n",
    "    'additional income',\n",
    "    'home-based',\n",
    "    'dig up dirt',\n",
    "    'double your',\n",
    "    'earn extra',\n",
    "    'extra cash',\n",
    "    'expect to earn',\n",
    "    'fast cash',\n",
    "    'free access',\n",
    "    'free investment',\n",
    "    'free membership',\n",
    "    'free offer',\n",
    "    'free preview',\n",
    "    'increase sales',\n",
    "    'increase traffic',\n",
    "    'internet marketing',\n",
    "    'marketing solution',\n",
    "    'multi-level marketing',\n",
    "    'online biz opportunity',\n",
    "    'remove',\n",
    "    'search engine',\n",
    "    'this won’t believe',\n",
    "    'winner',\n",
    "    'winning',\n",
    "    'work from home',\n",
    "    'you are a winner!',\n",
    "    'your income',\n",
    "    'your family'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018da54e",
   "metadata": {},
   "source": [
    "### Create function to classify each sentence in the paragraph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9874bbcc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for sentence in sentences:\n",
    "    # Preprocess the sentence\n",
    "    sentence_vectorized = vectorizer.transform([sentence])\n",
    "    \n",
    "    # Predict whether the sentence is spam or not\n",
    "    prediction = clf.predict(sentence_vectorized)\n",
    "    \n",
    "    # Count words in the sentence\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    word_count = len(words)\n",
    "    \n",
    "    # Identify spam words in the sentence\n",
    "    spam_words_in_sentence = [word.lower() for word in words if word.lower() in spam_words]\n",
    "    spam_word_count = len(spam_words_in_sentence)\n",
    "    \n",
    "    # Define a prediction result\n",
    "    prediction_result = \"spam\" if prediction[0] == 1 else \"not spam\"\n",
    "    \n",
    "    # Store information about the sentence\n",
    "    sentence_info.append({\n",
    "        'sentence': sentence,\n",
    "        'word_count': word_count,\n",
    "        'spam_word_count': spam_word_count,\n",
    "        'prediction_result': prediction_result,\n",
    "        'spam_words': spam_words_in_sentence,\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c98626c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: colorama in c:\\users\\hp\\appdata\\roaming\\python\\python311\\site-packages (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install colorama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3607adbf",
   "metadata": {},
   "source": [
    "### Let's time to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f73912e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define ANSI escape codes for text formatting\n",
    "COLORS = {\n",
    "    'spam': '\\033[91m',  # Red\n",
    "    'not spam': '\\033[92m',  # Green\n",
    "}\n",
    "\n",
    "ITALIC = '\\033[3m'  # Italic\n",
    "RESET = '\\033[0m'  # Reset text formatting\n",
    "\n",
    "# Define emojis\n",
    "EMOJIS = {\n",
    "    'spam': '😞',  # Sad face emoji\n",
    "    'not spam': '😃',  # Happy face emoji\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6108780e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: My Grandfather smoked his whole life.\n",
      "Word Count: 7\n",
      "Spam Word Count: 0\n",
      "Prediction Result: \u001b[91mSPAM\u001b[0m 😞\n",
      "Spam Words: \u001b[3m\u001b[0m\n",
      "\n",
      "\n",
      "Sentence: I was about 10 years old when my mother said to him, 'If you ever want to see your grandchildren graduate, you have to stop immediately.'.\n",
      "Word Count: 31\n",
      "Spam Word Count: 1\n",
      "Prediction Result: \u001b[91mSPAM\u001b[0m 😞\n",
      "Spam Words: \u001b[3mstop\u001b[0m\n",
      "\n",
      "\n",
      "Sentence: Tears welled up in his eyes when he realized what exactly was at stake.\n",
      "Word Count: 15\n",
      "Spam Word Count: 0\n",
      "Prediction Result: \u001b[91mSPAM\u001b[0m 😞\n",
      "Spam Words: \u001b[3m\u001b[0m\n",
      "\n",
      "\n",
      "Sentence: He gave it up immediately.\n",
      "Word Count: 6\n",
      "Spam Word Count: 0\n",
      "Prediction Result: \u001b[92mNOT SPAM\u001b[0m 😃\n",
      "\n",
      "\n",
      "Sentence: Three years later he died of lung cancer.\n",
      "Word Count: 9\n",
      "Spam Word Count: 0\n",
      "Prediction Result: \u001b[91mSPAM\u001b[0m 😞\n",
      "Spam Words: \u001b[3m\u001b[0m\n",
      "\n",
      "\n",
      "Sentence: It was really sad and destroyed me.\n",
      "Word Count: 8\n",
      "Spam Word Count: 0\n",
      "Prediction Result: \u001b[92mNOT SPAM\u001b[0m 😃\n",
      "\n",
      "\n",
      "Sentence: My mother said to me- 'Don't ever smoke.\n",
      "Word Count: 10\n",
      "Spam Word Count: 0\n",
      "Prediction Result: \u001b[91mSPAM\u001b[0m 😞\n",
      "Spam Words: \u001b[3m\u001b[0m\n",
      "\n",
      "\n",
      "Sentence: Please don't put your family through what your Grandfather put us through.\n",
      "Word Count: 14\n",
      "Spam Word Count: 0\n",
      "Prediction Result: \u001b[91mSPAM\u001b[0m 😞\n",
      "Spam Words: \u001b[3m\u001b[0m\n",
      "\n",
      "\n",
      "Sentence: I agreed.\n",
      "Word Count: 3\n",
      "Spam Word Count: 0\n",
      "Prediction Result: \u001b[91mSPAM\u001b[0m 😞\n",
      "Spam Words: \u001b[3m\u001b[0m\n",
      "\n",
      "\n",
      "Sentence: At 28, I have never touched a cigarette.\n",
      "Word Count: 10\n",
      "Spam Word Count: 0\n",
      "Prediction Result: \u001b[91mSPAM\u001b[0m 😞\n",
      "Spam Words: \u001b[3m\u001b[0m\n",
      "\n",
      "\n",
      "Sentence: I must say, I feel a very slight sense of regret for never having done it, because your post gave me cancer anyway.\n",
      "Word Count: 26\n",
      "Spam Word Count: 0\n",
      "Prediction Result: \u001b[92mNOT SPAM\u001b[0m 😃\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Iterate through the sentence_info list\n",
    "for info in sentence_info:\n",
    "    prediction_result = info['prediction_result']\n",
    "    \n",
    "    # Determine the text formatting and emoji based on the prediction result\n",
    "    if prediction_result in COLORS:\n",
    "        formatted_result = COLORS[prediction_result] + prediction_result.upper() + RESET\n",
    "        emoji = EMOJIS[prediction_result]\n",
    "    else:\n",
    "        formatted_result = prediction_result.upper()\n",
    "        emoji = ''\n",
    "    \n",
    "    # Print information with formatting and emoji\n",
    "    print(\"Sentence:\", info['sentence'])\n",
    "    print(\"Word Count:\", info['word_count'])\n",
    "    print(\"Spam Word Count:\", info['spam_word_count'])\n",
    "    print(\"Prediction Result:\", formatted_result + ' ' + emoji)\n",
    "    \n",
    "    if prediction_result == 'spam':\n",
    "        spam_words = ', '.join(info['spam_words'])\n",
    "        print(\"Spam Words:\", ITALIC + spam_words + RESET)\n",
    "    \n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7c8dc8",
   "metadata": {},
   "source": [
    "### Now save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5e7cb7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6d3af94a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['paragraph_naive_bayes_model.pkl']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the classifier and vectorizer to files\n",
    "joblib.dump(clf, 'paragraph_naive_bayes_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4b56027a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['paragraph_count_vectorizer.pkl']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(vectorizer, 'paragraph_count_vectorizer.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ebcb14ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also save the spam_words list for reference, if needed\n",
    "with open('paragraph_spam_words.txt', 'w') as file:\n",
    "    for word in spam_words:\n",
    "        file.write(word + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "349b4265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved classifier and vectorizer\n",
    "loaded_clf = joblib.load('paragraph_naive_bayes_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2ccb77c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_vectorizer = joblib.load('paragraph_count_vectorizer.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b621ddc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the spam_words list if needed\n",
    "with open('paragraph_spam_words.txt', 'r') as file:\n",
    "    loaded_spam_words = [line.strip() for line in file]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
